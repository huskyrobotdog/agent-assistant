use llama_cpp_2::context::params::LlamaContextParams;
use llama_cpp_2::llama_backend::LlamaBackend;
use llama_cpp_2::llama_batch::LlamaBatch;
use llama_cpp_2::model::params::LlamaModelParams;
use llama_cpp_2::model::{AddBos, LlamaChatMessage, LlamaModel, Special};
use llama_cpp_2::sampling::LlamaSampler;
use parking_lot::RwLock;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::num::NonZeroU32;
use std::path::PathBuf;
use std::sync::Arc;
use thiserror::Error;

/// Agent é”™è¯¯ç±»å‹
#[derive(Error, Debug)]
pub enum AgentError {
    #[error("æ¨¡å‹åŠ è½½å¤±è´¥: {0}")]
    ModelLoadError(String),
    #[error("æ¨ç†é”™è¯¯: {0}")]
    InferenceError(String),
    #[error("å·¥å…·æ‰§è¡Œé”™è¯¯: {0}")]
    ToolExecutionError(String),
    #[error("è§£æé”™è¯¯: {0}")]
    ParseError(String),
    #[error("MCP é”™è¯¯: {0}")]
    McpError(String),
}

/// MCP å·¥å…·å®šä¹‰
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct McpTool {
    pub name: String,
    pub description: String,
    pub input_schema: serde_json::Value,
}

/// MCP å·¥å…·è°ƒç”¨è¯·æ±‚
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ToolCall {
    pub name: String,
    pub arguments: serde_json::Value,
}

/// MCP å·¥å…·è°ƒç”¨ç»“æœ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ToolResult {
    pub tool_name: String,
    pub result: String,
    pub is_error: bool,
}

/// Agent æ¶ˆæ¯è§’è‰²
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "lowercase")]
pub enum Role {
    System,
    User,
    Assistant,
    Tool,
}

/// Agent æ¶ˆæ¯
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Message {
    pub role: Role,
    pub content: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_calls: Option<Vec<ToolCall>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_call_id: Option<String>,
}

/// Agent é…ç½®
#[derive(Debug, Clone)]
pub struct AgentConfig {
    pub model_path: PathBuf,
    pub n_ctx: u32,
    pub n_threads: i32,
    pub n_gpu_layers: u32,
    pub temperature: f32,
    pub top_p: f32,
    pub top_k: i32,
    pub min_p: f32,
    pub presence_penalty: f32,
    pub max_tokens: i32,
    pub seed: u32,
}

impl Default for AgentConfig {
    fn default() -> Self {
        Self {
            model_path: PathBuf::from("models/Qwen3-4B-Thinking-2507-UD-IQ1_M.gguf"),
            n_ctx: 8192,
            n_threads: 4,
            n_gpu_layers: 99,
            temperature: 0.6,
            top_p: 0.95,
            top_k: 20,
            min_p: 0.0,            // Qwen3 æ¨è 0.0
            presence_penalty: 1.0, // Qwen3 å»ºè®® â‰¤ 2.0ï¼Œé™ä½ä»¥ä¿æŒè¾“å‡ºè´¨é‡
            max_tokens: 4096,
            seed: 1234,
        }
    }
}

/// ReAct Agent çŠ¶æ€
#[derive(Debug, Clone, PartialEq)]
pub enum AgentState {
    Idle,
    Thinking,
    Acting,
    Observing,
    Finished,
    Error,
}

/// Agent ç”Ÿæˆå›è°ƒ
pub type GenerationCallback = Box<dyn Fn(&str) + Send + Sync>;

/// Agent ç”Ÿæˆå›è°ƒå¼•ç”¨
pub type GenerationCallbackRef<'a> = Option<&'a dyn Fn(&str)>;

/// å·¥å…·ç»“æœå›è°ƒå¼•ç”¨
pub type ToolResultCallbackRef<'a> = Option<&'a dyn Fn(&str, &str, bool)>;

/// MCP å·¥å…·æ‰§è¡Œå™¨ trait
pub trait McpToolExecutor: Send + Sync {
    fn execute(&self, tool_call: &ToolCall) -> Result<ToolResult, AgentError>;
    fn get_tools(&self) -> Vec<McpTool>;
}

/// ReAct Agent å®ç°
pub struct ReactAgent {
    backend: LlamaBackend,
    model: LlamaModel,
    config: AgentConfig,
    messages: RwLock<Vec<Message>>,
    tools: RwLock<Vec<McpTool>>,
    tool_executors: RwLock<HashMap<String, Arc<dyn McpToolExecutor>>>,
    state: RwLock<AgentState>,
}

impl ReactAgent {
    /// åˆ›å»ºæ–°çš„ Agent
    pub fn new(config: AgentConfig) -> Result<Self, AgentError> {
        // ç¦ç”¨ llama æ—¥å¿—
        let log_options = llama_cpp_2::LogOptions::default().with_logs_enabled(false);
        llama_cpp_2::send_logs_to_tracing(log_options);

        let backend = LlamaBackend::init()
            .map_err(|e| AgentError::ModelLoadError(format!("åˆå§‹åŒ– llama åç«¯å¤±è´¥: {}", e)))?;

        let model_params = LlamaModelParams::default().with_n_gpu_layers(config.n_gpu_layers);

        let model = LlamaModel::load_from_file(&backend, &config.model_path, &model_params)
            .map_err(|e| AgentError::ModelLoadError(format!("åŠ è½½æ¨¡å‹å¤±è´¥: {}", e)))?;

        Ok(Self {
            backend,
            model,
            config,
            messages: RwLock::new(Vec::new()),
            tools: RwLock::new(Vec::new()),
            tool_executors: RwLock::new(HashMap::new()),
            state: RwLock::new(AgentState::Idle),
        })
    }

    /// æ³¨å†Œ MCP å·¥å…·æ‰§è¡Œå™¨
    pub fn register_tool_executor(&self, name: &str, executor: Arc<dyn McpToolExecutor>) {
        let mut executors = self.tool_executors.write();
        let mut tools = self.tools.write();

        for tool in executor.get_tools() {
            tools.push(tool);
        }
        executors.insert(name.to_string(), executor);
    }

    /// è®¾ç½®ç³»ç»Ÿæç¤ºè¯
    pub fn set_system_prompt(&self, prompt: &str) {
        let mut messages = self.messages.write();
        messages.retain(|m| m.role != Role::System);
        messages.insert(
            0,
            Message {
                role: Role::System,
                content: prompt.to_string(),
                tool_calls: None,
                tool_call_id: None,
            },
        );
    }

    /// æ„å»º Hermes-style ç³»ç»Ÿæç¤ºè¯ (Qwen3 æ¨èæ ¼å¼)
    fn build_react_system_prompt(&self) -> String {
        let tools = self.tools.read();
        let tools_json = if tools.is_empty() {
            "[]".to_string()
        } else {
            let tools_array: Vec<serde_json::Value> = tools
                .iter()
                .map(|t| {
                    serde_json::json!({
                        "type": "function",
                        "function": {
                            "name": t.name,
                            "description": t.description,
                            "parameters": t.input_schema
                        }
                    })
                })
                .collect();
            serde_json::to_string_pretty(&tools_array).unwrap_or_default()
        };

        format!(
            r#"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.

<tools>
{tools_json}
</tools>

For each function call, return a JSON object with function name and arguments within <tool_call></tool_call> XML tags:

<tool_call>
{{"name": "function_name", "arguments": {{"param": "value"}}}}
</tool_call>

After calling a tool, you will receive the result within <tool_response></tool_response> XML tags.

Guidelines:
- Call only ONE tool at a time
- Wait for tool results in <tool_response> before proceeding
- If a tool fails, try alternative approaches
- Provide concise final answers
- Always respond in the same language as the user's query"#,
            tools_json = tools_json
        )
    }

    /// æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    pub fn add_user_message(&self, content: &str) {
        #[cfg(debug_assertions)]
        println!("\nğŸ’¬ [ç”¨æˆ·è¾“å…¥] {}", content);

        let mut messages = self.messages.write();
        messages.push(Message {
            role: Role::User,
            content: content.to_string(),
            tool_calls: None,
            tool_call_id: None,
        });
    }

    /// æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
    fn build_prompt(&self) -> Result<String, AgentError> {
        let messages = self.messages.read();

        let template = self
            .model
            .chat_template(None)
            .map_err(|e| AgentError::InferenceError(format!("è·å– chat template å¤±è´¥: {}", e)))?;

        let chat_messages: Result<Vec<_>, _> = messages
            .iter()
            .map(|m| {
                let role = match m.role {
                    Role::System => "system",
                    Role::User => "user",
                    Role::Assistant => "assistant",
                    Role::Tool => "tool",
                };
                LlamaChatMessage::new(role.to_string(), m.content.clone())
                    .map_err(|e| AgentError::InferenceError(format!("åˆ›å»ºæ¶ˆæ¯å¤±è´¥: {}", e)))
            })
            .collect();

        let chat_messages = chat_messages?;

        self.model
            .apply_chat_template(&template, &chat_messages, true)
            .map_err(|e| AgentError::InferenceError(format!("åº”ç”¨ chat template å¤±è´¥: {}", e)))
    }

    /// ç”Ÿæˆå›å¤
    pub fn generate(&self) -> Result<String, AgentError> {
        self.generate_with_callback(None)
    }

    /// ç”Ÿæˆå›å¤ï¼ˆå¸¦å›è°ƒï¼‰
    pub fn generate_with_callback(
        &self,
        callback: Option<&dyn Fn(&str)>,
    ) -> Result<String, AgentError> {
        *self.state.write() = AgentState::Thinking;

        #[cfg(debug_assertions)]
        println!("\nğŸ§  [å¼€å§‹æ¨ç†]");

        let prompt = self.build_prompt()?;

        #[cfg(debug_assertions)]
        {
            let char_count = prompt.chars().count();
            println!("\nğŸ“ [Prompt é•¿åº¦] {} å­—ç¬¦", char_count);
            // æ‰“å° prompt çš„æœ€å 200 ä¸ªå­—ç¬¦ï¼ˆé¿å…è¾“å‡ºè¿‡å¤šï¼‰
            if char_count > 200 {
                let tail: String = prompt.chars().skip(char_count - 200).collect();
                println!("\nğŸ“ [Prompt æœ«å°¾] ...{}", tail);
            } else {
                println!("\nğŸ“ [Prompt] {}", prompt);
            }
        }

        let ctx_params = LlamaContextParams::default()
            .with_n_ctx(Some(NonZeroU32::new(self.config.n_ctx).unwrap()))
            .with_n_threads(self.config.n_threads)
            .with_n_threads_batch(self.config.n_threads);

        let mut ctx = self
            .model
            .new_context(&self.backend, ctx_params)
            .map_err(|e| AgentError::InferenceError(format!("åˆ›å»ºä¸Šä¸‹æ–‡å¤±è´¥: {}", e)))?;

        let tokens = self
            .model
            .str_to_token(&prompt, AddBos::Never) // chat template å·²æ·»åŠ  BOS
            .map_err(|e| AgentError::InferenceError(format!("åˆ†è¯å¤±è´¥: {}", e)))?;

        let mut batch = LlamaBatch::new(self.config.n_ctx as usize, 1);

        let last_index = tokens.len() as i32 - 1;
        for (i, token) in tokens.iter().enumerate() {
            let is_last = i as i32 == last_index;
            batch
                .add(*token, i as i32, &[0], is_last)
                .map_err(|e| AgentError::InferenceError(format!("æ·»åŠ  token å¤±è´¥: {}", e)))?;
        }

        ctx.decode(&mut batch)
            .map_err(|e| AgentError::InferenceError(format!("è§£ç å¤±è´¥: {}", e)))?;

        // Qwen3 æ¨èé‡‡æ ·é¡ºåº: temp â†’ top_k â†’ top_p â†’ min_p â†’ dist
        let mut sampler = LlamaSampler::chain_simple([
            LlamaSampler::penalties(
                64,                           // æƒ©ç½šçª—å£å¤§å°
                1.1,                          // repeat_penalty
                0.0,                          // frequency_penalty
                self.config.presence_penalty, // presence_penalty
            ),
            LlamaSampler::temp(self.config.temperature),
            LlamaSampler::top_k(self.config.top_k),
            LlamaSampler::top_p(self.config.top_p, 1),
            LlamaSampler::min_p(self.config.min_p, 1), // Qwen3 æ¨è 0.0
            LlamaSampler::dist(self.config.seed),
        ]);

        let mut output = String::new();
        let mut n_cur = batch.n_tokens();
        let mut decoder = encoding_rs::UTF_8.new_decoder();

        while n_cur < self.config.max_tokens {
            let token = sampler.sample(&ctx, batch.n_tokens() - 1);
            sampler.accept(token);

            if self.model.is_eog_token(token) {
                break;
            }

            let token_bytes = self
                .model
                .token_to_bytes(token, Special::Tokenize)
                .map_err(|e| AgentError::InferenceError(format!("è½¬æ¢ token å¤±è´¥: {}", e)))?;

            let mut token_str = String::with_capacity(32);
            let _ = decoder.decode_to_string(&token_bytes, &mut token_str, false);

            output.push_str(&token_str);

            #[cfg(debug_assertions)]
            {
                use std::io::Write;
                print!("{}", token_str);
                let _ = std::io::stdout().flush();
            }

            if let Some(cb) = callback {
                cb(&token_str);
            }

            batch.clear();
            batch
                .add(token, n_cur, &[0], true)
                .map_err(|e| AgentError::InferenceError(format!("æ·»åŠ  token å¤±è´¥: {}", e)))?;

            ctx.decode(&mut batch)
                .map_err(|e| AgentError::InferenceError(format!("è§£ç å¤±è´¥: {}", e)))?;

            n_cur += 1;
        }

        #[cfg(debug_assertions)]
        println!(
            "\n\nâœ… [æ¨ç†å®Œæˆ] å…±ç”Ÿæˆ {} ä¸ª token",
            n_cur - tokens.len() as i32
        );

        *self.state.write() = AgentState::Idle;
        Ok(output)
    }

    /// è§£æå·¥å…·è°ƒç”¨ (æ”¯æŒå¤šç§æ ¼å¼)
    fn parse_tool_calls(&self, response: &str) -> Vec<ToolCall> {
        let mut tool_calls = Vec::new();

        // æ ¼å¼ 1: <tool_call>...</tool_call> (Hermes-styleï¼Œæ”¯æŒå¤šè¡Œ JSON)
        let re = regex::Regex::new(r"(?s)<tool_call>\s*(\{.*?\})\s*</tool_call>").ok();
        if let Some(re) = re {
            for cap in re.captures_iter(response) {
                if let Some(json_str) = cap.get(1) {
                    // æ¸…ç† JSON å­—ç¬¦ä¸²ä¸­çš„æ¢è¡Œå’Œå¤šä½™ç©ºæ ¼
                    let cleaned = json_str.as_str().trim();
                    if let Ok(tool_call) = serde_json::from_str::<ToolCall>(cleaned) {
                        tool_calls.push(tool_call);
                    }
                }
            }
        }

        // æ ¼å¼ 2: Qwen é£æ ¼çš„ function call (å¤‡ç”¨)
        if tool_calls.is_empty() {
            let qwen_re =
                regex::Regex::new(r#"(?s)âœ¿FUNCTIONâœ¿:\s*(\w+)\s*\nâœ¿ARGSâœ¿:\s*(\{.*?\})(?:\n|$)"#)
                    .ok();
            if let Some(re) = qwen_re {
                for cap in re.captures_iter(response) {
                    if let (Some(name), Some(args)) = (cap.get(1), cap.get(2)) {
                        if let Ok(arguments) = serde_json::from_str(args.as_str().trim()) {
                            tool_calls.push(ToolCall {
                                name: name.as_str().to_string(),
                                arguments,
                            });
                        }
                    }
                }
            }
        }

        // æ ¼å¼ 3: ç›´æ¥ JSON å¯¹è±¡ (æœ€åå¤‡ç”¨)
        if tool_calls.is_empty() {
            let json_re = regex::Regex::new(
                r#"(?s)\{\s*"name"\s*:\s*"([^"]+)"\s*,\s*"arguments"\s*:\s*(\{.*?\})\s*\}"#,
            )
            .ok();
            if let Some(re) = json_re {
                for cap in re.captures_iter(response) {
                    if let (Some(name), Some(args)) = (cap.get(1), cap.get(2)) {
                        if let Ok(arguments) = serde_json::from_str(args.as_str().trim()) {
                            tool_calls.push(ToolCall {
                                name: name.as_str().to_string(),
                                arguments,
                            });
                        }
                    }
                }
            }
        }

        tool_calls
    }

    /// æå–æ€è€ƒå†…å®¹
    #[allow(dead_code)]
    fn extract_thinking(&self, response: &str) -> Option<String> {
        let re = regex::Regex::new(r"<think>([\s\S]*?)</think>").ok()?;
        re.captures(response)
            .and_then(|cap| cap.get(1))
            .map(|m| m.as_str().trim().to_string())
    }

    /// æ‰§è¡Œå•æ¬¡ ReAct å¾ªç¯
    pub fn step(&self) -> Result<(String, bool), AgentError> {
        self.step_with_callbacks(None, None)
    }

    /// æ‰§è¡Œå•æ¬¡ ReAct å¾ªç¯ï¼ˆå¸¦å›è°ƒï¼‰
    pub fn step_with_callback(
        &self,
        callback: Option<&dyn Fn(&str)>,
    ) -> Result<(String, bool), AgentError> {
        self.step_with_callbacks(callback, None)
    }

    /// æ‰§è¡Œå•æ¬¡ ReAct å¾ªç¯ï¼ˆå¸¦ç”Ÿæˆå›è°ƒå’Œå·¥å…·ç»“æœå›è°ƒï¼‰
    pub fn step_with_callbacks(
        &self,
        callback: Option<&dyn Fn(&str)>,
        tool_callback: Option<&dyn Fn(&str, &str, bool)>,
    ) -> Result<(String, bool), AgentError> {
        #[cfg(debug_assertions)]
        println!("\nğŸ”„ [ReAct Step] å¼€å§‹æ‰§è¡Œå•æ¬¡å¾ªç¯");

        let response = self.generate_with_callback(callback)?;

        let tool_calls = self.parse_tool_calls(&response);

        #[cfg(debug_assertions)]
        if !tool_calls.is_empty() {
            println!("\nğŸ”§ [æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨] {:?}", tool_calls);
        }

        if !tool_calls.is_empty() {
            *self.state.write() = AgentState::Acting;

            {
                let mut messages = self.messages.write();
                messages.push(Message {
                    role: Role::Assistant,
                    content: response.clone(),
                    tool_calls: Some(tool_calls.clone()),
                    tool_call_id: None,
                });
            }

            for tool_call in &tool_calls {
                let result = self.execute_tool(tool_call)?;

                // é€šè¿‡å›è°ƒå‘é€å·¥å…·æ‰§è¡Œç»“æœ
                if let Some(cb) = tool_callback {
                    cb(&result.tool_name, &result.result, result.is_error);
                }

                let mut messages = self.messages.write();
                messages.push(Message {
                    role: Role::Tool,
                    content: format!("å·¥å…· {} æ‰§è¡Œç»“æœ:\n{}", result.tool_name, result.result),
                    tool_calls: None,
                    tool_call_id: Some(tool_call.name.clone()),
                });
            }

            *self.state.write() = AgentState::Observing;
            Ok((response, false))
        } else {
            {
                let mut messages = self.messages.write();
                messages.push(Message {
                    role: Role::Assistant,
                    content: response.clone(),
                    tool_calls: None,
                    tool_call_id: None,
                });
            }

            *self.state.write() = AgentState::Finished;
            Ok((response, true))
        }
    }

    /// æ‰§è¡Œå·¥å…·è°ƒç”¨
    fn execute_tool(&self, tool_call: &ToolCall) -> Result<ToolResult, AgentError> {
        #[cfg(debug_assertions)]
        println!(
            "\nâš¡ [æ‰§è¡Œå·¥å…·] {} å‚æ•°: {}",
            tool_call.name, tool_call.arguments
        );

        let executor_opt = {
            let executors = self.tool_executors.read();
            executors
                .iter()
                .find(|(_, executor)| {
                    executor
                        .get_tools()
                        .iter()
                        .any(|t| t.name == tool_call.name)
                })
                .map(|(_, executor)| executor.clone())
        };

        if let Some(executor) = executor_opt {
            let result = executor.execute(tool_call);
            #[cfg(debug_assertions)]
            if let Ok(ref r) = result {
                println!("\nğŸ“¤ [å·¥å…·ç»“æœ] {}: {}", r.tool_name, r.result);
            }
            return result;
        }

        #[cfg(debug_assertions)]
        println!("\nâŒ [å·¥å…·æœªæ‰¾åˆ°] {}", tool_call.name);

        Ok(ToolResult {
            tool_name: tool_call.name.clone(),
            result: format!("å·¥å…· {} æœªæ‰¾åˆ°", tool_call.name),
            is_error: true,
        })
    }

    /// è¿è¡Œå®Œæ•´çš„ ReAct å¾ªç¯
    pub fn run(&self, user_input: &str, max_iterations: usize) -> Result<String, AgentError> {
        self.run_with_callbacks(user_input, max_iterations, None, None)
    }

    /// è¿è¡Œå®Œæ•´çš„ ReAct å¾ªç¯ï¼ˆå¸¦å›è°ƒï¼‰
    pub fn run_with_callback(
        &self,
        user_input: &str,
        max_iterations: usize,
        callback: Option<&dyn Fn(&str)>,
    ) -> Result<String, AgentError> {
        self.run_with_callbacks(user_input, max_iterations, callback, None)
    }

    /// è¿è¡Œå®Œæ•´çš„ ReAct å¾ªç¯ï¼ˆå¸¦ç”Ÿæˆå›è°ƒå’Œå·¥å…·ç»“æœå›è°ƒï¼‰
    pub fn run_with_callbacks(
        &self,
        user_input: &str,
        max_iterations: usize,
        callback: Option<&dyn Fn(&str)>,
        tool_callback: Option<&dyn Fn(&str, &str, bool)>,
    ) -> Result<String, AgentError> {
        #[cfg(debug_assertions)]
        println!("\n\nğŸš€ ================== ReAct Agent å¼€å§‹ ==================");
        #[cfg(debug_assertions)]
        println!("ğŸ“Š [æœ€å¤§è¿­ä»£æ¬¡æ•°] {}", max_iterations);

        if self.messages.read().is_empty()
            || !self.messages.read().iter().any(|m| m.role == Role::System)
        {
            self.set_system_prompt(&self.build_react_system_prompt());
            #[cfg(debug_assertions)]
            println!("ğŸ“‹ [ç³»ç»Ÿæç¤ºè¯å·²è®¾ç½®]");
        }

        self.add_user_message(user_input);

        let mut final_response = String::new();
        let mut iterations = 0;

        loop {
            if iterations >= max_iterations {
                #[cfg(debug_assertions)]
                println!("\nâš ï¸ [è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°] {}", max_iterations);
                break;
            }

            #[cfg(debug_assertions)]
            println!("\nğŸ” [è¿­ä»£] {}/{}", iterations + 1, max_iterations);

            let (response, is_done) = self.step_with_callbacks(callback, tool_callback)?;

            final_response = response;

            if is_done {
                #[cfg(debug_assertions)]
                println!("\nâœ… [ä»»åŠ¡å®Œæˆ]");
                break;
            }

            iterations += 1;
        }

        #[cfg(debug_assertions)]
        println!("\nğŸ ================== ReAct Agent ç»“æŸ ==================\n");

        Ok(final_response)
    }

    /// æ¸…ç©ºå¯¹è¯å†å²
    pub fn clear_history(&self) {
        let mut messages = self.messages.write();
        messages.retain(|m| m.role == Role::System);
    }

    /// è·å–å½“å‰çŠ¶æ€
    pub fn get_state(&self) -> AgentState {
        self.state.read().clone()
    }

    /// è·å–å¯¹è¯å†å²
    pub fn get_messages(&self) -> Vec<Message> {
        self.messages.read().clone()
    }
}

/// å†…ç½®çš„ Echo å·¥å…·æ‰§è¡Œå™¨ï¼ˆç”¨äºæµ‹è¯•ï¼‰
pub struct EchoToolExecutor;

impl McpToolExecutor for EchoToolExecutor {
    fn execute(&self, tool_call: &ToolCall) -> Result<ToolResult, AgentError> {
        Ok(ToolResult {
            tool_name: tool_call.name.clone(),
            result: format!(
                "Echo: {}",
                serde_json::to_string(&tool_call.arguments).unwrap()
            ),
            is_error: false,
        })
    }

    fn get_tools(&self) -> Vec<McpTool> {
        vec![McpTool {
            name: "echo".to_string(),
            description: "å›æ˜¾è¾“å…¥çš„å†…å®¹".to_string(),
            input_schema: serde_json::json!({
                "type": "object",
                "properties": {
                    "message": {
                        "type": "string",
                        "description": "è¦å›æ˜¾çš„æ¶ˆæ¯"
                    }
                },
                "required": ["message"]
            }),
        }]
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_agent_config_default() {
        let config = AgentConfig::default();
        assert_eq!(config.n_ctx, 8192);
        assert_eq!(config.temperature, 0.6);
    }

    #[test]
    fn test_parse_tool_calls() {
        let _response = r#"
æˆ‘éœ€è¦è°ƒç”¨å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚
<tool_call>
{"name": "echo", "arguments": {"message": "hello"}}
</tool_call>
"#;

        let _agent = ReactAgent::new(AgentConfig {
            model_path: PathBuf::from("test.gguf"),
            ..Default::default()
        });

        // æ³¨æ„ï¼šè¿™ä¸ªæµ‹è¯•åœ¨æ²¡æœ‰å®é™…æ¨¡å‹æ—¶ä¼šå¤±è´¥
        // å®é™…ä½¿ç”¨æ—¶éœ€è¦æœ‰æ•ˆçš„æ¨¡å‹æ–‡ä»¶
    }
}
